# Deep Dive into Gemini CLI’s Core Module (ReAct Loop & Tools) and Porting to Tinkerbell

## Gemini CLI Core Overview – ReAct Loop and Architecture

**Gemini CLI** is an open-source AI agent that runs in the terminal and uses a **Reason and Act (ReAct)** loop to tackle tasks like coding, debugging, content generation, etc.. At its core, Gemini CLI orchestrates an LLM (Google’s *Gemini* model) with a set of tools and a **system prompt** to enable complex workflows. The **core module** (in `packages/core/src/core`) implements this logic. Key elements of the Gemini CLI core include:

* **System Prompt & Rules:** A comprehensive system prompt defines the agent’s role, tone, and strict guidelines. For example, it instructs the AI that *“You are an interactive CLI agent specializing in software engineering tasks”* and lays out *Core Mandates* (e.g. follow project conventions, verify library usage, mimic code style). The prompt also specifies how the agent should behave (be concise, no chit-chat) and includes rules for safety/security and tool usage. For instance, it warns *“NEVER talk to the user or describe your changes through comments”* when editing code. The prompt even provides default technology preferences when none are specified (e.g. *“CLIs: Python or Go.”* for command-line apps). This system prompt is fed into the model at startup to ground the agent’s behavior.

* **Built-in Tools:** Gemini CLI extends the model with **tools** that perform actions on the local system or internet. The core module defines a *BaseTool* interface and a **Tool Registry** for managing these tools. Each tool has: a unique `name` (used by the model to invoke it), a `description` for the model, a JSON `parameterSchema`, and `execute()` logic (plus methods for validation and confirmation). The standard **built-in tools** include:

    * *File System Tools:* `LSTool` (list directory contents), `ReadFileTool`, `WriteFileTool`, `GrepTool` (search file content), `GlobTool` (find files by pattern), `EditTool` (in-place text replacement in files, used for code modifications), and `ReadManyFilesTool` (read multiple files at once). These allow the agent to examine and modify the project’s files (with safeguards like operating in a root directory sandbox).
    * *Execution Tools:* `ShellTool` which runs shell commands on the system (with precautions like explaining dangerous commands and requiring user confirmation). This lets the AI compile code, run tests, start servers, etc., as needed.
    * *Web Tools:* `WebFetchTool` (HTTP GET a URL’s content) and `WebSearchTool` (perform a Google search via the Gemini API) for pulling in external information.
    * *Memory Tool:* `MemoryTool` to store/retrieve user-specific facts or preferences (a persistent memory across sessions).

  All these tools are registered at startup, and their availability (names, descriptions, schemas) is exposed to the Gemini model as *function definitions*. This means the model knows what tools it can call and how to call them. (Internally, Gemini CLI uses a Chat Completion with function calling: the tools are presented as functions, so the model can return a `FunctionCall` in its output to invoke one.)

## The “Reason-Act” Loop in Gemini CLI Core

The core module implements a loop where the model **reasons** about the user’s request, possibly uses tools (acts), then continues reasoning with the results. This is often referred to as the ReAct loop. Here’s how it works in Gemini CLI:

1. **User Prompt & Model Reasoning:** The user enters a request or query in the CLI. The conversation (including the system prompt and dialogue history) is fed to the Gemini model. The model “reasons” about how to fulfill the request. Based on its reasoning and the provided tool schemas, it may decide that it needs to use a tool (e.g. to read a file, search the codebase, run a command, etc.).

2. **Model Proposes a Tool (Function Call):** If the model determines an action is needed, it outputs a function/tool call instead of a final answer. For example, the model might respond with a *FunctionCall* indicating `{"name": "search_file_content", "arguments": {...}}` to grep the code, or `{"name": "run_shell_command", "arguments": {...}}` to execute a shell command (in the chat transcript, these appear as `[tool_call: ...]`). This is the “Act” part of the loop – the model is requesting the agent core to perform an action.

3. **Core Interprets the Tool Call:** The core intercepts this function call and looks up the corresponding Tool in its registry. It parses the arguments and validates them via the tool’s schema and `validateToolParams()` method.

4. **User Confirmation (if required):** Before executing, the core checks if the tool deems the action sensitive. Each tool’s `shouldConfirmExecute()` can signal that confirmation is needed (for instance, a destructive file edit or a shell command). If so, Gemini CLI will prompt the user to approve or cancel the specific action. (In the CLI UI, the user sees a description of the pending action and must confirm it.) Only if the user approves does the loop proceed. This ensures safety and user control.

5. **Tool Execution:** The core then invokes the tool’s `execute()` method, performing the actual action in the environment. For example, running the shell command, reading the file content, etc. The tool returns a **ToolResult** object which typically contains: (a) `llmContent` – the raw result meant for the model (e.g. file text, command output, or a structured summary), and (b) `returnDisplay` – a user-friendly output to show in the terminal UI. The `returnDisplay` might be formatted (Markdown or diff output) for the user’s benefit, while `llmContent` is often a concise factual summary or data for the model’s context.

6. **Model Receives Tool Result:** The core wraps the tool’s result content into a *FunctionResponse* and feeds it back to the LLM as the next turn of the conversation. Essentially, the model now has the outcome of the tool action available as context (in a consistent format). For instance, if the model searched for a function name in the code, it now sees the matching lines from grep. The conversation continues, with the model now able to “reason” again using this new information.

7. **Iterate Reason-Act as Needed:** The model can now either produce the final answer or decide further actions. Often, the LLM will chain multiple tool uses. **Gemini CLI’s system prompt encourages a structured approach**: e.g., first *“Understand”* by reading/searching code, then *“Plan”* (sometimes share a plan with user), then *“Implement”* changes via edit/write tools, then *“Verify”* by running tests or linters. The core loop supports this by allowing the model to call tools repeatedly. Each tool result is fed back, and the model’s next response is awaited. This cycle continues until the model outputs a final **user-facing answer** (an ordinary message with no function call). At that point, the agent responds to the user in the CLI with that answer, completing the task (or it may await further user input, if interactive). Throughout, the system prompt’s rules (like staying concise, avoiding irrelevant chatter, etc.) guide the model’s outputs.

8. **Example:** As an illustration, if a user says *“Refactor the auth logic in src/auth.py to use the requests library instead of urllib.”*, the model might respond by planning and using tools step by step. In a sample from Gemini CLI, the model replied: *“Okay, I’ll analyze the code first.”* then did `[tool_call: GlobTool for 'tests/test_auth.py']` followed by reading the test file. After seeing tests exist, it confirmed `requests` is in dependencies by `[tool_call: ReadFileTool for '.../requirements.txt']`. Then it outlined a plan (replace urllib, add try/except, run tests) and asked user *“Should I proceed?”*. Upon user approval, the model used an edit/write tool to apply changes and a shell tool to run linter and tests. This kind of multi-turn reasoning and acting is exactly what the core’s ReAct loop enables. The final answer to the user is the completed refactor confirmation. (All these intermediate steps were managed by the core loop, with user confirming the critical ones.)

**Summary:** The core module essentially acts as the orchestrator between the **LLM** and the **environment**. It provides the rules (prompt) and tools, receives the model’s intentions (function calls), and carries them out with the user’s oversight. This design lets Gemini CLI handle complex tasks that require reading/writing files or executing code, all while keeping the LLM within safe guardrails. The result is a powerful “AI pair programmer” in the terminal that can autonomously perform multi-step coding tasks, but always with an audit trail and user in control of actual side-effects.

## Porting Gemini Core Functionality to Tinkerbell (Rust)

Now, as systems engineers, we need to replicate Gemini CLI’s core functionality “like-for-like” in **Tinkerbell**, our Rust-based project (let’s call our agent module `lsyour` for now). The goal for the first pass is to **match Gemini’s capabilities and prompts** as closely as possible, but implemented in Rust and adapted to Tinkerbell’s design paradigms. Below is a breakdown of how we can do this:

* **Recreating the ReAct Loop:** We will implement a similar **conversation loop** in Rust. We’ll have an initial system prompt (based on Gemini’s prompt) and feed it into our LLM of choice. The LLM’s outputs will be parsed to check if it’s a tool invocation or a direct answer. In practice, we can use an API that supports function calls (for example, OpenAI GPT-4 function calling, or Google’s Gemini API if accessible). If using OpenAI’s API, we’d define functions for each tool with a JSON schema (mirroring Gemini’s `parameterSchema`), so the model can call them. If using an open-source model without native function-call support, we can instruct the model to output a special format (like the bracketed `[tool_call: ...]` syntax that Gemini’s prompt used) and then parse it. The logic will be:

    1. Send user prompt + system prompt to the model (and any chat history).
    2. If model returns a function/tool call, parse the tool name and parameters.
    3. Look up the tool and validate params. If confirmation is required, pause to ask the user in the CLI (this can be as simple as a y/n prompt in our Rust CLI).
    4. Execute the tool action in Rust, capture the result.
    5. Feed the result back to the model (as a function result or by appending to the prompt context), then loop back to step 1 to let the model continue reasoning.
    6. When the model finally produces a normal message (no tool call), output it to the user.

  This flow is essentially the same as Gemini’s core loop. In Rust, we might implement this loop with an async function that interacts with the LLM API and our tool executors in sequence. Ensuring that the model’s “thought” (reasoning) stays hidden and only the final answers or confirmations are shown to the user will be important for a clean UX (just like Gemini CLI does).

* **Porting the Tool System:** We need to create Rust equivalents for the various **tools** Gemini provides. This suggests designing a **Tool trait/struct** similar to `BaseTool` in Gemini CLI. Each tool in Rust should have: a `name` (string identifier), a `description` (for the model’s knowledge), a parameter specification (we can define a struct or use `serde_json::Value` with a schema), an `execute(&self, params) -> ToolResult` method, and perhaps a flag or method to indicate if confirmation is needed. We can then maintain a global registry or dictionary of tools in the agent. Key tools to implement in the first pass:

    * **File operations:**

        * **List Directory**: A Rust tool (analogous to `LSTool`) that lists files/folders. We can use Rust’s `std::fs::read_dir` for implementation, filtering out ignored patterns.
        * **Read File**: Read file content given a path. We’ll need to handle text vs binary. For text, read up to some max size; for images/PDF, possibly return a placeholder or base64 (though an initial version might skip complex binary support).
        * **Write File**: Write given content to a file (create or overwrite). Straightforward using `std::fs::write`.
        * **Search (Grep)**: Search file contents for a regex. In Rust, we could invoke `grep` externally or use the `regex` crate to scan files. For performance, if the workspace is a git repo, we might attempt to call `git grep` as Gemini does, otherwise do manual search. We’ll format results similarly (file path and line snippets).
        * **Glob Find**: Find files by a glob pattern. We can use the `glob` crate or `ignore` crate (which respects .gitignore) to implement this, returning a sorted list of file paths (likely sorted by mod time as Gemini does).
        * **Edit/Replace**: This is a bit tricky – Gemini’s `EditTool` does context-validated replacement. For our first pass, we might implement a simpler version: take a file path, an old string, and new string and perform one replacement (with maybe an exact match requirement). Eventually, we’d want the robust multi-line context matching approach that Gemini uses to avoid wrong replacements. But a basic version can be done with reading the file to a string and using Rust’s `replace` function, then writing it back, assuming the occurrence is unique.

    * **Execution tool:**

        * **Shell Command**: We will implement a `ShellTool` analogous to `run_shell_command`. In Rust, that means using `std::process::Command` to run commands. We should enforce the same safety measures: execute in a controlled directory (e.g., the project root), and **require confirmation** for commands that can modify state (we can flag any command containing `rm `, `git push`, etc., or more simply always ask confirmation unless it’s a read-only command like `ls`). We’ll capture stdout, stderr, exit code, and present a summary to the model and user. For background processes (`&`), we might support starting them and returning immediately (storing the PID). Many of these details (like how to handle interactive commands or background tasks) are spelled out in Gemini’s guidelines, so we should aim to match those behaviors. Using Rust’s async I/O or threads can help run commands without blocking the main loop.

    * **Web access tools:**

        * **Web Fetch**: We can use an HTTP client crate (e.g. `reqwest`) to fetch a URL’s content. The result (if text or HTML) can be returned to the model. If binary, perhaps indicate it can’t be displayed (or base64 encode if needed).
        * **Web Search**: Implementing a web search might be non-trivial due to API access. Google’s search is not publicly open; Gemini CLI likely uses a special API via the model (the *Grounding with Google Search* integration). For Tinkerbell’s first pass, we might integrate with an API like Bing Web Search or SerpAPI to retrieve search results. Or we can omit this in v1 if needed and add it later. If included, a `WebSearchTool` would take a query string, call the search API, and return a short list of top results (title and snippet) for the model to use.

    * **Memory tool:**

        * We can provide a `MemoryTool` that allows the model to save a piece of information (e.g., a key-value store). In Rust, this could write to a local JSON or a small database. For now, an in-memory static map or a file in the project directory can suffice. The idea is to persist facts between sessions (for example, if the user says “Remember my preferred coding style is K\&R”), the model can call `MemoryTool` to save it. Later queries could retrieve it. Implement `MemoryTool` with subcommands like `remember` and `recall` or a parameter indicating whether to store or fetch.

  We will create each of these tools as a struct implementing a common trait (with an `execute`). We’ll register them in a `ToolRegistry` (perhaps just a `HashMap<String, Box<dyn Tool>>`). For exposing to the model, if using OpenAI API, we’ll construct a function definition (name, description, JSON schema) from each tool’s metadata and pass that to the chat completion call. If using a pure prompt approach, we might list the tools in the system prompt with usage instructions (though that’s less structured). The structured approach is preferable for accuracy.

* **Using Gemini’s Prompts in Tinkerbell:** We will **reuse Gemini’s prompt content** as much as possible, tweaking it for our environment. The Gemini system prompt is essentially a well-crafted policy and strategy document for the AI agent. We can take this entire prompt (it’s Apache-2.0 licensed in the repo) and transplant it into our Rust agent, adjusting only a few details: for example, where it says the agent’s name or refers to “Gemini”, we can rename it to our agent’s identity (Tinkerbell). We might also adjust tool names in the prompt if they differ (Gemini used placeholders like `${GrepTool.Name}` which our implementation might change to e.g. `search_file_content` function name). The core guidance (code conventions, not assuming libraries, how to comment, how to confirm with user, etc.) should remain the same because it’s exactly the behavior we want to emulate. This prompt will ensure our agent behaves similarly “like-for-like” to Gemini CLI in reasoning and style. It’s a huge advantage to start with these proven instructions rather than writing our own from scratch. We should still test and iterate on the prompt once integrated, but the first pass is to carry it over directly.

* **Rust Modules and Dependencies:** To implement the above, we will likely introduce a few new modules/crates in our Rust project:

    * An **LLM client module** (for calling the model API). If using OpenAI, we can use the `openai` crate or just direct REST calls. For Google’s Gemini API, if available, we may need to call it via HTTP as well (they might have REST endpoints or gRPC – this needs investigation beyond this scope).
    * A **tools module** defining the Tool trait and all tool implementations (file\_tools, shell\_tool, web\_tools, memory\_tool, etc.). We might break it into submodules for organization (similar to Gemini’s structure under `core/src/tools`).
    * Use of async runtime (Tokio) to allow parallel operations – for instance, the prompt advises performing independent searches in parallel. In our Rust agent, we could indeed spawn multiple file reads or search tasks concurrently (e.g., using `tokio::join!`). In the first simple pass, we might do them sequentially, but Rust’s strength in concurrency means we should plan to leverage it soon.
    * Crates: `serde_json` for handling JSON schemas and tool parameters, `regex` or external command for grep, `glob` or `ignore` for file globbing, `reqwest` for HTTP, perhaps `tokio` for async processes and timeouts (to avoid tools hanging).
    * For the **“memory” persistence**, possibly the `sled` or `sqlite` crate if we want a durable store, or simply writing to a JSON file in the working directory for now.

* **Adapting to Tinkerbell’s Paradigm:** Tinkerbell might have its own architectural patterns (for example, maybe it’s more standalone or integrated with an IDE, etc.). In our design, we should ensure the core logic is abstracted so it can be used in different contexts (e.g., in a CLI app, or maybe as a daemon). Rust encourages a clear separation between the library (core logic) and the binary (UI/CLI). We can implement the agent core as a Rust library (`lsyour_core`) and then have a thin CLI wrapper that uses it. This mirrors how Gemini CLI’s core is separate from its Ink-powered terminal UI. For Tinkerbell, if there’s a specific framework or style (perhaps Tinkerbell might integrate with some editor or be part of a bigger system), we should keep the design flexible. But fundamentally, the agent loop and tools are the core, which we can encapsulate in our Rust project in a similar way to Gemini’s core package.

## Future Improvements and Async Scheduling (Planned)

We will **leave more advanced concurrency and scheduling mechanisms open for now**, as those will be tackled after the first pass. Notably, the team has mentioned using a *“David Beazley’s PyOS8 scheduler”*-inspired approach for cooperative multitasking with coroutines in Rust. The idea there is to manage multiple tool calls or background tasks in a turn-based fashion, improving efficiency (for example, the agent could initiate a long-running tool in the background and continue with other reasoning). Rust doesn’t have built-in green threads in the same way, but we can use async/await and perhaps a custom executor to achieve similar effects. This could allow our Tinkerbell agent to interleave I/O-bound tasks or handle multiple user queries at once in the future. For now, our focus is on a faithful single-task ReAct loop. We will design the code in a way that adding such a scheduler later is possible (e.g., making tool executions async, and designing the loop to handle re-entrant or concurrent calls).

**In summary**, our first port will mirror Gemini CLI’s core: using the *same prompts* (adjusted for naming), implementing an equivalent set of tools in Rust, and reproducing the reason-act loop with confirmation steps. This gives us a solid baseline agent with comparable functionality to Gemini CLI. Once this is in place, we can iteratively enhance it within Tinkerbell – adding the advanced coroutine scheduler, introducing any project-specific tools or memory features, and generally polishing the integration. The result will be a Rust-powered “Gemini-like” AI assistant, aligned with Tinkerbell’s needs and ready to evolve beyond the initial like-for-like functionality.

**Sources:**

* Google Cloud – *Gemini CLI uses a ReAct loop with tools*
* Simon Willison’s Blog – *Gemini CLI system prompt and tool capabilities*
* Gemini CLI Documentation (haleclipse) – *Core Tools API and built-in tools list*
* Gemini CLI System Prompt (excerpt) and Examples for ReAct loop illustration.
